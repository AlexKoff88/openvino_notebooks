{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Quantize NLP models with Post-Training Optimization Tool ​in OpenVINO™\n",
    "This tutorial demonstrates how to improve performance of sparse NLP models with [OpenVINO](https://docs.openvino.ai/) on 4th Gen Intel® Xeon® Scalable processors. It uses a pre-trained model from the [HuggingFace Transformers](https://huggingface.co/transformers/) library and shows how to convert it to the OpenVINO™ IR format and run inference of the model on the CPU using a dedicated runtime option that enables sparsity optimizations. It also demonstrates how to get more performance stacking sparsity with 8-bit quantization. To simplify the user experience, the [HuggingFace Optimum](https://huggingface.co/docs/optimum) library is used to convert the model to the OpenVINO™ IR format and quantize it. It consists of the following steps:\n",
    "\n",
    "- Download and convert the sparse BERT model.\n",
    "- Compare sparse vs. dense inference performance.\n",
    "- Quantize model.\n",
    "- Compare sparse 8-bit vs. dense 8-bit inference performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771388d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "import openvino.runtime as ov\n",
    "\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "from optimum.intel.openvino import OVQuantizer\n",
    "from optimum.intel.openvino import OVConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc335d",
   "metadata": {
    "id": "YytHDzLE0uOJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9fc64c",
   "metadata": {
    "id": "f7i6dWUmhloy"
   },
   "outputs": [],
   "source": [
    "model_id = \"neuralmagic/oBERT-12-downstream-pruned-unstructured-90-mnli\"\n",
    "local_path = \"./bert_90_sparse\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = OVModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "model.save_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cfef2",
   "metadata": {},
   "source": [
    "Instantiate a model using OpenVINO Python API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d874166",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "ov_model = core.read_model(local_path + \"/openvino_model.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63247a5",
   "metadata": {},
   "source": [
    "## Prepare model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5b8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great restaurant. This is a great restaurant. This is a great restaurant. This is a great restaurant. This is a great restaurant\"\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "ov_inputs = {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"token_type_ids\": inputs[\"token_type_ids\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca2fa0",
   "metadata": {
    "id": "ehX7F6KB0uPu"
   },
   "source": [
    "## Benchmark dense inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2f6d66",
   "metadata": {
    "id": "r5as0_Yg0uQX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense model median elapsed time: 0.009908080101013184\n"
     ]
    }
   ],
   "source": [
    "dense_compiled = core.compile_model(ov_model, \"CPU\")\n",
    "\n",
    "attempts = 1000\n",
    "dense_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = dense_compiled(ov_inputs)\n",
    "    dense_counters.append(time.time() - m_start)\n",
    "\n",
    "dense_median = np.median(np.array(dense_counters))\n",
    "\n",
    "print(f\"Dense model median elapsed time: {dense_median}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1633e9",
   "metadata": {},
   "source": [
    "## Bechmark sparse inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025e5add",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[ NOT_FOUND ] Unsupported property SPARSE_WEIGHTS_DECOMPRESSION_RATE by CPU plugin",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-18247d50ff2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#core2.set_property(\"CPU\", ov.properties.intel_cpu.sparse_weights_decompression_rate(0.8))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"SPARSE_WEIGHTS_DECOMPRESSION_RATE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msparse_compiled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mov_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msparse_counters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virt_envs/ov_notebooks/lib/python3.8/site-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(self, model, device_name, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         return CompiledModel(\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         )\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [ NOT_FOUND ] Unsupported property SPARSE_WEIGHTS_DECOMPRESSION_RATE by CPU plugin"
     ]
    }
   ],
   "source": [
    "#core2 = ov.Core()\n",
    "#core2.set_property(\"CPU\", ov.properties.intel_cpu.sparse_weights_decompression_rate(0.8))\n",
    "config = {\"SPARSE_WEIGHTS_DECOMPRESSION_RATE\": 0.8}\n",
    "sparse_compiled = core.compile_model(ov_model, \"CPU\", config)\n",
    "\n",
    "sparse_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = sparse_compiled(ov_inputs)\n",
    "    sparse_counters.append(time.time() - m_start)\n",
    "\n",
    "sparse_median = np.median(np.array(sparse_counters))\n",
    "\n",
    "print(f\"Sparse model median elapsed time: {sparse_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603a481",
   "metadata": {},
   "source": [
    "## Quantize model with HuggingFace Optimum API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b897c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8deeadbb598a4a6a8a52f26063beb2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Loading cached shuffled indices for dataset at /home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2bba8406484faf80.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea8ef97a5b644a1815ba2cddec97498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/ov_notebooks/lib/python3.8/site-packages/nncf/torch/quantization/quantize_functions.py:134: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  return g.op(add_domain(\"FakeQuantize\"), input_, input_low, input_high, output_low, output_high, levels_i=levels)\n",
      "/home/alex/virt_envs/ov_notebooks/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(\n",
      "/home/alex/virt_envs/ov_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/alex/virt_envs/ov_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"./bert_90_sparse_quantized\"\n",
    "\n",
    "torch_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", max_length=128, truncation=True\n",
    "    )\n",
    "\n",
    "# Load the default quantization configuration detailing the quantization we wish to apply\n",
    "quantization_config = OVConfig()\n",
    "# Instantiate our OVQuantizer using the desired configuration\n",
    "quantizer = OVQuantizer.from_pretrained(torch_model, feature=\"sequence-classification\")\n",
    "# Create the calibration dataset used to perform static quantization\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"glue\",\n",
    "    dataset_config_name=\"mnli\",\n",
    "    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
    "    num_samples=100,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "# Apply static quantization and export the resulting quantized model to OpenVINO IR format\n",
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config, calibration_dataset=calibration_dataset, save_directory=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830eb7",
   "metadata": {},
   "source": [
    "## Benchmark quantized dense inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ov_model = core.read_model(save_dir + \"/openvino_model.xml\")\n",
    "q_dense_compiled = core.compile_model(q_ov_model, \"CPU\")\n",
    "\n",
    "attempts = 1000\n",
    "q_dense_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = q_dense_compiled(ov_inputs)\n",
    "    q_dense_counters.append(time.time() - m_start)\n",
    "\n",
    "q_dense_median = np.median(np.array(q_dense_counters))\n",
    "\n",
    "print(f\"Dense model median elapsed time: {q_dense_median}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ov_notebooks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe409241748dff4afe127d33bbdaaa11b54ce82261ed669fd8a0538ea98c62f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
