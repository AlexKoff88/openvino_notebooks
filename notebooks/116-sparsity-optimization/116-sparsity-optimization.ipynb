{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Accelerate Inference of sparse Transformer models with OpenVINO™ and 4th Gen Intel&reg; Xeon&reg; Scalable processors\n",
    "This tutorial demonstrates how to improve performance of sparse Transformer models with [OpenVINO](https://docs.openvino.ai/) on 4th Gen Intel® Xeon® Scalable processors. It uses a pre-trained model from the [HuggingFace Transformers](https://huggingface.co/transformers/) library and shows how to convert it to the OpenVINO™ IR format and run inference of the model on the CPU using a dedicated runtime option that enables sparsity optimizations. It also demonstrates how to get more performance stacking sparsity with 8-bit quantization. To simplify the user experience, the [HuggingFace Optimum](https://huggingface.co/docs/optimum) library is used to convert the model to the OpenVINO™ IR format and quantize it. It consists of the following steps:\n",
    "\n",
    "- Download and quantize sparse BERT model from the public using HuggingFace Optimum for OpenVINO.\n",
    "- Compare sparse 8-bit vs. dense 8-bit inference performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771388d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from optimum.intel.openvino import OVQuantizer\n",
    "from optimum.intel.openvino import OVConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603a481",
   "metadata": {},
   "source": [
    "## Quantize model with HuggingFace Optimum API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"neuralmagic/oBERT-12-downstream-pruned-unstructured-90-mnli\"\n",
    "quantized_sparse_dir = Path(\"bert_90_sparse_quantized\")\n",
    "\n",
    "torch_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", max_length=128, truncation=True\n",
    "    )\n",
    "\n",
    "quantization_config = OVConfig()\n",
    "quantizer = OVQuantizer.from_pretrained(torch_model, feature=\"sequence-classification\")\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"glue\",\n",
    "    dataset_config_name=\"mnli\",\n",
    "    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
    "    num_samples=100,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "# Apply static quantization and export the resulting quantized model to OpenVINO IR format\n",
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config, calibration_dataset=calibration_dataset, save_directory=quantized_sparse_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830eb7",
   "metadata": {},
   "source": [
    "## Benchmark quantized dense inference performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307b51f",
   "metadata": {},
   "source": [
    "Benchmark dense inference performance using parallel execution on four CPU cores. Sequense length is 32 which fits to everage Ssquense length of popular benchmark datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa895f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "with open(\"perf_config.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "\"\"\"\n",
    "{\n",
    "    \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4}\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m bert_90_sparse_quantized/openvino_model.xml -shape \"input_ids[1,32],attention_mask[1,32],token_type_ids[1,32]\" -load_config perf_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9151b11",
   "metadata": {},
   "source": [
    "## Benchmark quantized sparse inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "# \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\" controls minimum sparsity rate for weights to consider \n",
    "# for sparse optimization at the runtime.\n",
    "with open(\"perf_config_sparse.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "\"\"\"\n",
    "{\n",
    "    \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4, \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\": 0.8}\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m bert_90_sparse_quantized/openvino_model.xml -shape \"input_ids[1,32],attention_mask[1,32],token_type_ids[1,32]\" -load_config perf_config_sparse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d4d61",
   "metadata": {},
   "source": [
    "## When this might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c8526",
   "metadata": {},
   "source": [
    "This feauture can improve inference performance for models with sparse weights in the scenarios when the model is deployed to handle multiple requests in parallel. application that processes multiple requests in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ov_notebooks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe409241748dff4afe127d33bbdaaa11b54ce82261ed669fd8a0538ea98c62f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
