{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Accelerate Inference of Sparse Transformer Models with OpenVINO™ and 4th Gen Intel&reg; Xeon&reg; Scalable Processors\n",
    "This tutorial demonstrates how to improve performance of sparse Transformer models with [OpenVINO](https://docs.openvino.ai/) on 4th Gen Intel® Xeon® Scalable processors. It uses a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/transformers/) library and shows how to convert it to the OpenVINO™ IR format and run inference on a CPU using a dedicated runtime option that enables sparsity optimizations. It also demonstrates how to get more performance stacking sparsity with 8-bit quantization. To simplify the user experience, the [Hugging Face Optimum](https://huggingface.co/docs/optimum) library is used to convert the model to OpenVINO™ IR format and quantize it using [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf). It consists of the following steps:\n",
    "\n",
    "- Install prerequisites\n",
    "- Download and quantize sparse BERT model from a public source using the OpenVINO integration with Hugging Face Optimum.\n",
    "- Compare sparse 8-bit vs. dense 8-bit inference performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bef22e9",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc9afb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[openvino] in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy<1.24.0 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (1.23.4)\n",
      "Requirement already satisfied: torch>=1.9 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (1.13.1)\n",
      "Requirement already satisfied: coloredlogs in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (15.0.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.20.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (4.25.1)\n",
      "Requirement already satisfied: sympy in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (1.11.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (0.11.1)\n",
      "Requirement already satisfied: packaging in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (22.0)\n",
      "Requirement already satisfied: optimum-intel[openvino] in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum[openvino]) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (4.4.0)\n",
      "Requirement already satisfied: requests in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (6.0)\n",
      "Requirement already satisfied: filelock in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from torch>=1.9->optimum[openvino]) (11.7.99)\n",
      "Requirement already satisfied: wheel in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->optimum[openvino]) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->optimum[openvino]) (47.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.20.1->optimum[openvino]) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.20.1->optimum[openvino]) (0.13.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.20.1->optimum[openvino]) (0.1.97)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.20.1->optimum[openvino]) (3.20.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from coloredlogs->optimum[openvino]) (10.0)\n",
      "Requirement already satisfied: scipy in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum-intel[openvino]->optimum[openvino]) (1.9.3)\n",
      "Requirement already satisfied: datasets in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum-intel[openvino]->optimum[openvino]) (2.8.0)\n",
      "Requirement already satisfied: openvino>=2022.2.0 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from optimum-intel[openvino]->optimum[openvino]) (2022.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from sympy->optimum[openvino]) (1.2.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (2022.11.0)\n",
      "Requirement already satisfied: xxhash in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (3.2.0)\n",
      "Requirement already satisfied: aiohttp in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (10.0.1)\n",
      "Requirement already satisfied: multiprocess in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (0.70.14)\n",
      "Requirement already satisfied: pandas in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from datasets->optimum-intel[openvino]->optimum[openvino]) (1.3.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum[openvino]) (1.26.13)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from aiohttp->datasets->optimum-intel[openvino]->optimum[openvino]) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from aiohttp->datasets->optimum-intel[openvino]->optimum[openvino]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from aiohttp->datasets->optimum-intel[openvino]->optimum[openvino]) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from aiohttp->datasets->optimum-intel[openvino]->optimum[openvino]) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from aiohttp->datasets->optimum-intel[openvino]->optimum[openvino]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from aiohttp->datasets->optimum-intel[openvino]->optimum[openvino]) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from pandas->datasets->optimum-intel[openvino]->optimum[openvino]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from pandas->datasets->optimum-intel[openvino]->optimum[openvino]) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets->optimum-intel[openvino]->optimum[openvino]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[openvino]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c6bb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nncf  # noqa: F401\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/openvinotoolkit/nncf.git#egg=nncf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771388d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/nncf/torch/__init__.py:23: UserWarning: NNCF provides best results with torch==1.9.1, while current torch version is 1.13.1+cu117 - consider switching to torch==1.9.1\n",
      "  warnings.warn(\"NNCF provides best results with torch=={bkc}, \"\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/nncf/torch/dynamic_graph/patch_pytorch.py:163: UserWarning: Not patching unique_dim since it is missing in this version of PyTorch\n",
      "  warnings.warn(\"Not patching {} since it is missing in this version of PyTorch\".format(op_name))\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/openvino/offline_transformations/__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future, use `openvino.runtime.passes` instead!\n",
      "  warnings.warn(\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from optimum.intel.openvino import OVQuantizer\n",
    "from optimum.intel.openvino import OVConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603a481",
   "metadata": {},
   "source": [
    "## Quantize model with HuggingFace Optimum API\n",
    "The sparsity acceleration MatMul operations is available only in the case when these operations are quantized into 8-bit precision. If the model is not quantized it can be done using either way availble for OpenVINO models. For more details refer [here](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html). In this tutorial we use the HuggingFace Optimum API to quantize the model. The HuggingFace Optimum API is a high-level API that allows to convert and quantize models from the HuggingFace Transformers library to the OpenVINO™ IR format. For more details refer to the [HuggingFace Optimum documentation](https://huggingface.co/docs/optimum/intel/optimization_ov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b897c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 5/5 [00:00<00:00, 526.42it/s]\n",
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2bba8406484faf80.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 26.37ba/s]\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/nncf/torch/quantization/quantize_functions.py:134: FutureWarning: 'torch.onnx._patch_torch._graph_op' is deprecated in version 1.13 and will be removed in version 1.14. Please note 'g.op()' is to be removed from torch.Graph. Please open a GitHub issue if you need this functionality..\n",
      "  return g.op(add_domain(\"FakeQuantize\"), input_, input_low, input_high, output_low, output_high, levels_i=levels)\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/torch/onnx/_patch_torch.py:81: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/root/akozlov/virt_envs/ov_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of org.openvinotoolkit::FakeQuantize type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"neuralmagic/oBERT-12-downstream-pruned-unstructured-90-mnli\"\n",
    "quantized_sparse_dir = Path(\"bert_90_sparse_quantized\")\n",
    "\n",
    "# Instantiate model and tokenizer in PyTorch and load them from the HF Hub\n",
    "torch_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Define a function that tokenizes the data and returns it in the format expected by the model.\n",
    "    \n",
    "    :param: examples: a dictionary containing the input data which are the items from caliration dataset.\n",
    "            tokenizer: a tokenizer object that is used to tokenize the text data.\n",
    "    :returns:\n",
    "            the data that can be fed directly to the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tokenizer(\n",
    "        examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", max_length=128, truncation=True\n",
    "    )\n",
    "\n",
    "# Create quantization config (default) and OVQuantizer\n",
    "# OVConfig is a wrapper class on top of NNCF config. \n",
    "# Use \"compression\" field to control quantization parameters\n",
    "# For more information about the parameters refer to NNCF GitHub documentatioin\n",
    "quantization_config = OVConfig()\n",
    "quantizer = OVQuantizer.from_pretrained(torch_model, feature=\"sequence-classification\")\n",
    "\n",
    "# Instantiate a dataset and convert it to calibration dataset using HF API\n",
    "# The latter one produces a model input\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"glue\",\n",
    "    dataset_config_name=\"mnli\",\n",
    "    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
    "    num_samples=100,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "# Apply static quantization and export the resulting quantized model to OpenVINO IR format\n",
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config, calibration_dataset=calibration_dataset, save_directory=quantized_sparse_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830eb7",
   "metadata": {},
   "source": [
    "## Benchmark quantized dense inference performance\n",
    "Benchmark dense inference performance using parallel execution on four CPU cores to simulate a small instance in the cloud infrastructure. Sequense length is set to 16 which is common for multiple use cases, e.g. conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa895f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "with open(\"perf_config.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "\"\"\"\n",
    "{\n",
    "    \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4}\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6c7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 127.32 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [?,3]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,16], 'attention_mask': [1,16], 'token_type_ids': [1,16]\n",
      "[ INFO ] Reshape model took 25.11 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [1,16]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [1,3]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1199.55 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 4\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'bfloat16'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'attention_mask'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input 'attention_mask' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 9.28 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            34984 iterations\n",
      "[ INFO ] Duration:         60011.07 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        5.93 ms\n",
      "[ INFO ]    Average:       6.85 ms\n",
      "[ INFO ]    Min:           4.96 ms\n",
      "[ INFO ]    Max:           30.45 ms\n",
      "[ INFO ] Throughput:   582.96 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m bert_90_sparse_quantized/openvino_model.xml -shape \"input_ids[1,16],attention_mask[1,16],token_type_ids[1,16]\" -load_config perf_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9151b11",
   "metadata": {},
   "source": [
    "## Benchmark quantized sparse inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad77ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "# \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\" controls minimum sparsity rate for weights to consider \n",
    "# for sparse optimization at the runtime.\n",
    "with open(\"perf_config_sparse.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "\"\"\"\n",
    "{\n",
    "    \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4, \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\": 0.8}\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddd8b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading OpenVINO Runtime\n",
      "[ INFO ] OpenVINO:\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] Device info:\n",
      "[ INFO ] CPU\n",
      "[ INFO ] Build ................................. 2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
      "[ INFO ] \n",
      "[ INFO ] \n",
      "[Step 3/11] Setting device configuration\n",
      "[ WARNING ] Performance hint was not explicitly specified in command line. Device(CPU) performance hint will be set to THROUGHPUT.\n",
      "[Step 4/11] Reading model files\n",
      "[ INFO ] Loading model files\n",
      "[ INFO ] Read model took 138.77 ms\n",
      "[ INFO ] Original model I/O parameters:\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [?,?]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [?,?]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [?,3]\n",
      "[Step 5/11] Resizing model to match image sizes and given batch\n",
      "[ INFO ] Model batch size: 1\n",
      "[ INFO ] Reshaping model: 'input_ids': [1,16], 'attention_mask': [1,16], 'token_type_ids': [1,16]\n",
      "[ INFO ] Reshape model took 25.14 ms\n",
      "[Step 6/11] Configuring input of the model\n",
      "[ INFO ] Model inputs:\n",
      "[ INFO ]     input_ids (node: input_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ]     attention_mask (node: attention_mask) : i64 / [...] / [1,16]\n",
      "[ INFO ]     token_type_ids (node: token_type_ids) : i64 / [...] / [1,16]\n",
      "[ INFO ] Model outputs:\n",
      "[ INFO ]     logits (node: logits) : f32 / [...] / [1,3]\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Compile model took 1381.49 ms\n",
      "[Step 8/11] Querying optimal runtime parameters\n",
      "[ INFO ] Model:\n",
      "[ INFO ]   NETWORK_NAME: torch_jit\n",
      "[ INFO ]   OPTIMAL_NUMBER_OF_INFER_REQUESTS: 4\n",
      "[ INFO ]   NUM_STREAMS: 4\n",
      "[ INFO ]   AFFINITY: Affinity.CORE\n",
      "[ INFO ]   INFERENCE_NUM_THREADS: 4\n",
      "[ INFO ]   PERF_COUNT: False\n",
      "[ INFO ]   INFERENCE_PRECISION_HINT: <Type: 'bfloat16'>\n",
      "[ INFO ]   PERFORMANCE_HINT: PerformanceMode.THROUGHPUT\n",
      "[ INFO ]   PERFORMANCE_HINT_NUM_REQUESTS: 0\n",
      "[Step 9/11] Creating infer requests and preparing input tensors\n",
      "[ WARNING ] No input files were given for input 'input_ids'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'attention_mask'!. This input will be filled with random values!\n",
      "[ WARNING ] No input files were given for input 'token_type_ids'!. This input will be filled with random values!\n",
      "[ INFO ] Fill input 'input_ids' with random values \n",
      "[ INFO ] Fill input 'attention_mask' with random values \n",
      "[ INFO ] Fill input 'token_type_ids' with random values \n",
      "[Step 10/11] Measuring performance (Start inference asynchronously, 4 inference requests, limits: 60000 ms duration)\n",
      "[ INFO ] Benchmarking in inference only mode (inputs filling are not included in measurement loop).\n",
      "[ INFO ] First inference took 6.94 ms\n",
      "[Step 11/11] Dumping statistics report\n",
      "[ INFO ] Count:            54408 iterations\n",
      "[ INFO ] Duration:         60007.37 ms\n",
      "[ INFO ] Latency:\n",
      "[ INFO ]    Median:        4.38 ms\n",
      "[ INFO ]    Average:       4.40 ms\n",
      "[ INFO ]    Min:           4.23 ms\n",
      "[ INFO ]    Max:           8.36 ms\n",
      "[ INFO ] Throughput:   906.69 FPS\n"
     ]
    }
   ],
   "source": [
    "!benchmark_app -m bert_90_sparse_quantized/openvino_model.xml -shape \"input_ids[1,16],attention_mask[1,16],token_type_ids[1,16]\" -load_config perf_config_sparse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d4d61",
   "metadata": {},
   "source": [
    "## When this might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c8526",
   "metadata": {},
   "source": [
    "This feauture can improve inference performance for models with sparse weights in the scenarios when the model is deployed to handle multiple requests in parallel asyncronously. It is especially helpful in the case of small sequence length, e.g. 32 and lower.\n",
    "\n",
    "For more details about the asynchronous inference with OpenVINO refer to the following documentation:\n",
    "- [Deployment Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_deployment_optimization_guide_common.html#doxid-openvino-docs-deployment-optimization-guide-common-1async-api)\n",
    "- [Inference Request API](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Infer_request.html#doxid-openvino-docs-o-v-u-g-infer-request-1in-out-tensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ov_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "25fad543ad216e1bba3a2237c544f0cea27ae49559091fac659fb84d6cd5fdd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
