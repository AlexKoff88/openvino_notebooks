{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Accelerate Inference of sparse Transformer models with OpenVINO™ and 4th Gen Intel&reg; Xeon&reg; Scalable processors\n",
    "This tutorial demonstrates how to improve performance of sparse Transformer models with [OpenVINO](https://docs.openvino.ai/) on 4th Gen Intel® Xeon® Scalable processors. It uses a pre-trained model from the [HuggingFace Transformers](https://huggingface.co/transformers/) library and shows how to convert it to the OpenVINO™ IR format and run inference of the model on the CPU using a dedicated runtime option that enables sparsity optimizations. It also demonstrates how to get more performance stacking sparsity with 8-bit quantization. To simplify the user experience, the [HuggingFace Optimum](https://huggingface.co/docs/optimum) library is used to convert the model to the OpenVINO™ IR format and quantize it. It consists of the following steps:\n",
    "\n",
    "- Download and convert the sparse BERT model.\n",
    "- Compare sparse vs. dense inference performance.\n",
    "- Quantize model.\n",
    "- Compare sparse 8-bit vs. dense 8-bit inference performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771388d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/virt_envs/stable_diffusion/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/alex/virt_envs/stable_diffusion/lib/python3.8/site-packages/openvino/offline_transformations/__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future, use `openvino.runtime.passes` instead!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n",
      "INFO:nncf:Compiling and loading extensions for quantization...\n",
      "INFO:nncf:Compiling and loading extensions for binarization...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "import openvino.runtime as ov\n",
    "\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "from optimum.intel.openvino import OVQuantizer\n",
    "from optimum.intel.openvino import OVConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc335d",
   "metadata": {
    "id": "YytHDzLE0uOJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be9fc64c",
   "metadata": {
    "id": "f7i6dWUmhloy"
   },
   "outputs": [],
   "source": [
    "model_id = \"neuralmagic/oBERT-12-downstream-pruned-unstructured-90-mnli\"\n",
    "sparse_path = Path(\"bert_90_sparse\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = OVModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "model.save_pretrained(sparse_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75cfef2",
   "metadata": {},
   "source": [
    "Instantiate a model using OpenVINO Python API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d874166",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "ov_model = core.read_model(sparse_path / \"openvino_model.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63247a5",
   "metadata": {},
   "source": [
    "## Prepare model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5b8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great restaurant. This is a great restaurant. This is a great restaurant. This is a great restaurant. This is a great restaurant\"\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "ov_inputs = {\n",
    "    \"input_ids\": inputs[\"input_ids\"],\n",
    "    \"attention_mask\": inputs[\"attention_mask\"],\n",
    "    \"token_type_ids\": inputs[\"token_type_ids\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca2fa0",
   "metadata": {
    "id": "ehX7F6KB0uPu"
   },
   "source": [
    "## Benchmark dense inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2f6d66",
   "metadata": {
    "id": "r5as0_Yg0uQX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense model median elapsed time: 0.009994864463806152\n"
     ]
    }
   ],
   "source": [
    "dense_compiled = core.compile_model(ov_model, \"CPU\")\n",
    "\n",
    "attempts = 1000\n",
    "dense_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = dense_compiled(ov_inputs)\n",
    "    dense_counters.append(time.time() - m_start)\n",
    "\n",
    "dense_median = np.median(np.array(dense_counters))\n",
    "\n",
    "print(f\"Dense model median elapsed time: {dense_median}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1633e9",
   "metadata": {},
   "source": [
    "## Bechmark sparse inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025e5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse model median elapsed time: 0.009945034980773926\n"
     ]
    }
   ],
   "source": [
    "config = {\"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\": 0.8}\n",
    "sparse_compiled = core.compile_model(ov_model, \"CPU\", config)\n",
    "\n",
    "sparse_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = sparse_compiled(ov_inputs)\n",
    "    sparse_counters.append(time.time() - m_start)\n",
    "\n",
    "sparse_median = np.median(np.array(sparse_counters))\n",
    "\n",
    "print(f\"Sparse model median elapsed time: {sparse_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603a481",
   "metadata": {},
   "source": [
    "## Quantize model with HuggingFace Optimum API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b897c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ] Found cached dataset glue (/home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Found cached dataset glue (/home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 5/5 [00:00<00:00, 446.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ] Found cached dataset glue (/home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Found cached dataset glue (/home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ WARNING ] Loading cached shuffled indices for dataset at /home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2bba8406484faf80.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Loading cached shuffled indices for dataset at /home/alex/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2bba8406484faf80.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 51.54ba/s]\n"
     ]
    }
   ],
   "source": [
    "quantized_sparse_dir = Path(\"bert_90_sparse_quantized\")\n",
    "\n",
    "torch_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"], examples[\"hypothesis\"], padding=\"max_length\", max_length=128, truncation=True\n",
    "    )\n",
    "\n",
    "quantization_config = OVConfig()\n",
    "quantizer = OVQuantizer.from_pretrained(torch_model, feature=\"sequence-classification\")\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"glue\",\n",
    "    dataset_config_name=\"mnli\",\n",
    "    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
    "    num_samples=100,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "# Apply static quantization and export the resulting quantized model to OpenVINO IR format\n",
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config, calibration_dataset=calibration_dataset, save_directory=quantized_sparse_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830eb7",
   "metadata": {},
   "source": [
    "## Benchmark quantized dense inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f6c7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense quantized model median elapsed time: 0.006369829177856445\n"
     ]
    }
   ],
   "source": [
    "q_ov_model = core.read_model(quantized_sparse_dir / \"openvino_model.xml\")\n",
    "q_dense_compiled = core.compile_model(q_ov_model, \"CPU\")\n",
    "\n",
    "attempts = 1000\n",
    "q_dense_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = q_dense_compiled(ov_inputs)\n",
    "    q_dense_counters.append(time.time() - m_start)\n",
    "\n",
    "q_dense_median = np.median(np.array(q_dense_counters))\n",
    "\n",
    "print(f\"Dense quantized model median elapsed time: {q_dense_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9151b11",
   "metadata": {},
   "source": [
    "## Benchmark quantized sparse inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddd8b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse quantized model median elapsed time: 0.0063517093658447266\n"
     ]
    }
   ],
   "source": [
    "q_sparse_compiled = core.compile_model(q_ov_model, \"CPU\", config)\n",
    "\n",
    "q_sparse_counters = []\n",
    "for i in range(attempts):\n",
    "    m_start = time.time()\n",
    "    output = q_sparse_compiled(ov_inputs)\n",
    "    q_sparse_counters.append(time.time() - m_start)\n",
    "\n",
    "q_sparse_median = np.median(np.array(q_sparse_counters))\n",
    "\n",
    "print(f\"Sparse quantized model median elapsed time: {q_sparse_median}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('stable_diffusion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7918409a64d3d4275e0103fc4443d9be5863d1df136c02ed032407c7ae821339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
